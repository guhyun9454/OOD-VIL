import os
import torch
import time
import datetime
import numpy as np
import torch.nn.functional as F
import math
from timm.utils import accuracy
from timm.models import create_model
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from utils import save_accuracy_heatmap, save_anomaly_histogram, save_confusion_matrix_plot, save_logits_statistics
from continual_datasets.dataset_utils import RandomSampleWrapper  
import matplotlib.pyplot as plt
from OODdetectors.ood_adapter import compute_ood_scores, SUPPORTED_METHODS
from torch.utils.data import DataLoader

def load_model(args):
    model = create_model(
        "vit_base_patch16_224",
        pretrained=args.pretrained,
        num_classes=args.num_classes,
    )
    
    # Linear probing 모드: backbone freeze, classifier만 학습
    if args.linear_probing:
        # 모든 파라미터를 freeze
        for param in model.parameters():
            param.requires_grad = False
        
        # classifier(head)만 학습 가능하게 설정
        if hasattr(model, 'head') and model.head is not None:
            for param in model.head.parameters():
                param.requires_grad = True
            print(f"Classifier layer만 학습 가능하게 설정되었습니다. (파라미터 수: {sum(p.numel() for p in model.head.parameters())})")
        else:
            print("Warning: 모델에 'head' 속성이 없거나 None입니다.")
    
    return model

class Engine:
    def __init__(self, model=None, device=None, class_mask=[], domain_list=[], args=None):
        """
        Args:
            model: 학습할 모델.
            device: torch.device 객체.
            class_mask: task별 클래스 마스크 (incremental scenario에 따른).
            domain_list: 각 task의 도메인 정보.
            args: argparse에서 전달된 인자 (num_tasks, epochs, print_freq 등 포함).
        """
        self.model = model
        self.device = device
        self.args = args
        self.class_mask = class_mask
        self.domain_list = domain_list
        self.num_tasks = args.num_tasks

    def train_one_epoch(self, model, criterion, data_loader, optimizer, device, epoch, args):
        model.train()
        total_loss = 0.0
        total_acc = 0.0
        total_samples = 0

        for batch_idx, (inputs, targets) in enumerate(data_loader):
            if self.args.develop and batch_idx > 20: break

            inputs = inputs.to(device)
            targets = targets.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            acc1 = accuracy(outputs, targets, topk=(1,))[0]
            batch_size = inputs.size(0)
            
            total_loss += loss.item() * batch_size
            total_acc += acc1.item() * batch_size
            total_samples += batch_size

            if batch_idx % args.print_freq == 0:
                current_avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
                current_avg_acc = total_acc / total_samples if total_samples > 0 else 0.0
                print(f"Epoch [{epoch+1}/{args.epochs}], Batch [{batch_idx}/{len(data_loader)}]: "
                    f"Loss = {loss.item():.4f}, Acc@1 = {acc1.item():.2f}, "
                    f"Running Avg Loss = {current_avg_loss:.4f}, Running Avg Acc = {current_avg_acc:.2f}")

        epoch_avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
        epoch_avg_acc = total_acc / total_samples if total_samples > 0 else 0.0
        return epoch_avg_loss, epoch_avg_acc

    def _oe_enabled(self, args) -> bool:
        """OE를 --ood_method 로 선택했을 때만 task마다 OE 파인튜닝을 수행."""
        ood_method = getattr(args, "ood_method", None)
        if not ood_method:
            return False
        ood_method = str(ood_method).upper()
        if ood_method == "ALL":
            return False
        methods = [m.strip().upper() for m in ood_method.split(",") if m.strip()]
        return "OE" in methods

    def oe_finetune_task(
        self,
        model,
        criterion,
        id_loader,
        oe_dataset,
        optimizer,
        device,
        task_id,
        args,
    ):
        """
        Outlier Exposure(OE) fine-tuning:
        loss = CE(ID) + λ * CE(Uniform, softmax(OE))
        """
        if oe_dataset is None:
            raise ValueError("OE fine-tuning을 위해서는 args.ood_dataset 및 OOD dataset 로드가 필요합니다.")

        # OOD(OE) loader (label은 사용하지 않음)
        oe_loader = DataLoader(
            oe_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=args.num_workers,
            pin_memory=True,
        )

        # 기존 optimizer를 재사용하되, OE 단계에서만 lr을 0.001로 고정하고 이후 복원
        old_lrs = [pg.get("lr", None) for pg in optimizer.param_groups]
        for pg in optimizer.param_groups:
            pg["lr"] = float(args.oe_ft_lr)

        model.train()
        for ft_epoch in range(int(args.oe_ft_epochs)):
            running_loss, running_id, running_oe = 0.0, 0.0, 0.0
            n_steps = 0
            oe_iter = iter(oe_loader)

            for batch_idx, (x_in, y_in) in enumerate(id_loader):
                if args.develop and batch_idx > 20:
                    break

                try:
                    x_oe, _ = next(oe_iter)
                except StopIteration:
                    oe_iter = iter(oe_loader)
                    x_oe, _ = next(oe_iter)

                x_in = x_in.to(device, non_blocking=True)
                y_in = y_in.to(device, non_blocking=True)
                x_oe = x_oe.to(device, non_blocking=True)

                logits_in = model(x_in)
                loss_id = criterion(logits_in, y_in)

                logits_oe = model(x_oe)
                logp_oe = F.log_softmax(logits_oe, dim=1)
                loss_oe = -logp_oe.mean(dim=1).mean()

                loss = loss_id + float(args.oe_lambda) * loss_oe

                optimizer.zero_grad(set_to_none=True)
                loss.backward()
                optimizer.step()

                bs = x_in.size(0)
                running_loss += loss.item() * bs
                running_id += loss_id.item() * bs
                running_oe += loss_oe.item() * bs
                n_steps += bs

            if n_steps > 0:
                print(
                    f"Task {task_id+1} | OE-FT Epoch [{ft_epoch+1}/{int(args.oe_ft_epochs)}] "
                    f"Loss={running_loss/n_steps:.4f} (ID={running_id/n_steps:.4f}, OE={running_oe/n_steps:.4f}) "
                    f"lr={optimizer.param_groups[0]['lr']:.6f}"
                )

        # lr 복원
        for pg, lr in zip(optimizer.param_groups, old_lrs):
            if lr is not None:
                pg["lr"] = lr

    def evaluate_task(self, model, data_loader, device, task_id, class_mask, args):
        """
        한 task에 대해 evaluation을 수행하며, 매 print_freq 배치마다 중간 결과를 출력합니다.
        """
        criterion = torch.nn.CrossEntropyLoss().to(device)
        model.eval()
        total_acc = 0.0
        total_loss = 0.0
        total_samples = 0

        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(data_loader):
                if args.develop and batch_idx > 20: break

                inputs = inputs.to(device)
                targets = targets.to(device)
                
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                acc1 = accuracy(outputs, targets, topk=(1,))[0]
                batch_size = inputs.size(0)
                
                total_acc += acc1.item() * batch_size
                total_loss += loss.item() * batch_size
                total_samples += batch_size

                if batch_idx % args.print_freq == 0:
                    running_avg_loss = total_loss / total_samples
                    running_avg_acc = total_acc / total_samples
                    print(f"Task {task_id+1}, Batch [{batch_idx}/{len(data_loader)}]: "
                        f"Running Avg Loss = {running_avg_loss:.2f}, Running Avg Acc@1 = {running_avg_acc:.2f}")

        avg_acc = total_acc / total_samples
        avg_loss = total_loss / total_samples
        print(f"Task {task_id+1}: Final Avg Loss = {avg_loss:.2f} | Final Avg Acc@1 = {avg_acc:.2f}")
        return avg_acc

    def evaluate_ood(self, model, id_datasets, ood_dataset, device, args, task_id=None):
        model.eval()
        
        # === New unified OOD evaluation (adapter 기반) ===
        ood_method = args.ood_method.upper()
        
        id_size, ood_size = len(id_datasets), len(ood_dataset)
        min_size = min(id_size, ood_size)
        if args.develop:
            min_size = 1000
        if args.ood_develop:
            min_size = args.ood_develop
        if args.verbose:
            print(f"ID dataset size: {id_size}, OOD dataset size: {ood_size}. Using {min_size} samples each for evaluation.")
        
        id_dataset_aligned = RandomSampleWrapper(id_datasets, min_size, args.seed) if id_size > min_size else id_datasets
        ood_dataset_aligned = RandomSampleWrapper(ood_dataset, min_size, args.seed) if ood_size > min_size else ood_dataset
        
        id_loader = torch.utils.data.DataLoader(id_dataset_aligned, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
        ood_loader = torch.utils.data.DataLoader(ood_dataset_aligned, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
        
        if ood_method == "ALL":
            methods = SUPPORTED_METHODS
        else:
            # 쉼표로 구분된 메소드들 처리
            methods = [method.strip().upper() for method in ood_method.split(',')]
            # 지원되지 않는 메소드 확인
            unsupported = [m for m in methods if m not in SUPPORTED_METHODS]
            if unsupported:
                raise ValueError(f"지원되지 않는 OOD 메소드: {unsupported}. 지원되는 메소드: {SUPPORTED_METHODS}")
        
        from sklearn import metrics
        results = {}
        
        for method in methods:
            id_scores, ood_scores = compute_ood_scores(method, model, id_loader, ood_loader, device)
            
            if args.verbose or args.wandb:
                hist_path = save_anomaly_histogram(id_scores.numpy(), ood_scores.numpy(), args, suffix=method.lower(), task_id=task_id)
                if args.wandb:
                    import wandb
                    wandb.log({f"Anomaly Histogram TASK {task_id}": wandb.Image(hist_path)})
            
            binary_labels = np.concatenate([np.ones(id_scores.shape[0]), np.zeros(ood_scores.shape[0])])
            all_scores = np.concatenate([id_scores.numpy(), ood_scores.numpy()])
            
            fpr, tpr, _ = metrics.roc_curve(binary_labels, all_scores, drop_intermediate=False)
            auroc = metrics.auc(fpr, tpr)
            idx_tpr95 = np.abs(tpr - 0.95).argmin()
            fpr_at_tpr95 = fpr[idx_tpr95]
            
            print(f"[{method}]: AUROC {auroc * 100:.2f}% | FPR@TPR95 {fpr_at_tpr95 * 100:.2f}%")
            if args.wandb:
                import wandb
                wandb.log({f"{method}_AUROC (↑)": auroc * 100, f"{method}_FPR@TPR95 (↓)": fpr_at_tpr95 * 100, "TASK": task_id})
            
            results[method] = {"auroc": auroc, "fpr_at_tpr95": fpr_at_tpr95, "scores": all_scores}
        
        return results
        # === End unified OOD evaluation ===

    def evaluate_till_now(self, model, data_loader, device, task_id, class_mask, acc_matrix, args):
        """
        현재까지의 모든 task에 대해 평가하고,
        A_last, A_avg, Forgetting 지표를 계산하여 출력합니다.
        """
        for t in range(task_id+1):
            acc_matrix[t, task_id] = self.evaluate_task(model, data_loader[t]['val'], device, t, class_mask, args)

        A_i = [np.mean(acc_matrix[:i+1, i]) for i in range(task_id+1)]
        A_last = A_i[-1]
        A_avg = np.mean(A_i)

        result_str = "[Average accuracy till task{}] A_last: {:.2f} A_avg: {:.2f}".format(task_id+1, A_last, A_avg)
        
        if task_id > 0:
            forgetting = np.mean((np.max(acc_matrix, axis=1) - acc_matrix[:, task_id])[:task_id])
            result_str += " Forgetting: {:.4f}".format(forgetting)
        else:
            forgetting = 0

        if args.wandb:
            import wandb
            wandb.log({"A_last (↑)": A_last, "A_avg (↑)": A_avg, "Forgetting (↓)": forgetting, "TASK": task_id})
        
        print(result_str)
        if args.verbose or args.wandb:
            sub_matrix = acc_matrix[:task_id+1, :task_id+1]
            result = np.where(np.triu(np.ones_like(sub_matrix, dtype=bool)), sub_matrix, np.nan)
            heatmap_path = save_accuracy_heatmap(result, task_id, args)
            if args.wandb:
                import wandb
                wandb.log({"Accuracy Heatmap": wandb.Image(heatmap_path)})
        
        return {"Acc@1": A_last}

    def train_and_evaluate(self, model, criterion, data_loader, optimizer, lr_scheduler, device, class_mask, args):
        """
        전체 incremental learning 과정을 수행합니다.
        각 task에 대해 지정된 epoch만큼 fine-tuning 후,
        지금까지의 task에 대해 평가합니다.
        """
        acc_matrix = np.zeros((args.num_tasks, args.num_tasks))
        for task_id in range(args.num_tasks):
            print(f"{f'Training on Task {task_id+1}/{args.num_tasks}':=^60}")
            train_start = time.time()
            for epoch in range(args.epochs):
                epoch_start = time.time()
                epoch_avg_loss, epoch_avg_acc = self.train_one_epoch(model, criterion, data_loader[task_id]['train'], optimizer, device, epoch, args)
                epoch_duration = time.time() - epoch_start  
                print(f"Epoch [{epoch+1}/{args.epochs}] Completed in {str(datetime.timedelta(seconds=int(epoch_duration)))}: Avg Loss = {epoch_avg_loss:.4f}, Avg Acc@1 = {epoch_avg_acc:.2f}")

                if lr_scheduler is not None:
                    lr_scheduler.step(epoch)
            train_duration = time.time() - train_start
            print(f"Task {task_id+1} training completed in {str(datetime.timedelta(seconds=int(train_duration)))}")

            # --- OE fine-tuning (optional) ---
            if self._oe_enabled(args):
                if not args.ood_dataset:
                    raise ValueError("OE를 사용하려면 --ood_dataset 을 지정해야 합니다.")
                oe_dataset = data_loader[-1].get("ood", None)
                print(f"{f'OE Fine-tuning (lambda={args.oe_lambda}, epochs={args.oe_ft_epochs}, lr={args.oe_ft_lr})':=^60}")
                self.oe_finetune_task(
                    model=model,
                    criterion=criterion,
                    id_loader=data_loader[task_id]["train"],
                    oe_dataset=oe_dataset,
                    optimizer=optimizer,
                    device=device,
                    task_id=task_id,
                    args=args,
                )

            print(f'{f"Testing on Task {task_id+1}/{args.num_tasks}":=^60}')
            eval_start = time.time()
            self.evaluate_till_now(model, data_loader, device, task_id, class_mask, acc_matrix, args)
            eval_duration = time.time() - eval_start
            print(f"Task {task_id+1} evaluation completed in {str(datetime.timedelta(seconds=int(eval_duration)))}")

            if args.ood_dataset:
                print(f"{f'OOD Evaluation':=^60}")
                ood_start = time.time()
                # 현재 task까지의 ID 데이터셋만 사용
                all_id_datasets = torch.utils.data.ConcatDataset([data_loader[t]['val'].dataset for t in range(task_id+1)])
                ood_loader = data_loader[-1]['ood']
                self.evaluate_ood(model, all_id_datasets, ood_loader, device, args, task_id)
                ood_duration = time.time() - ood_start
                print(f"OOD evaluation after Task {task_id+1} completed in {str(datetime.timedelta(seconds=int(ood_duration)))}")
                
            # 추가: 일반 추론 시의 logits 통계 저장 - 제거
            if args.save:
                checkpoint_dir = os.path.join(args.save, 'checkpoint')
                os.makedirs(checkpoint_dir, exist_ok=True)
                checkpoint_path = os.path.join(checkpoint_dir, 'task{}_checkpoint.pth'.format(task_id+1))
                checkpoint = {
                    'model': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'epoch': epoch,
                    'args': args,
                    'linear_probing': args.linear_probing,
                }
                with open(checkpoint_path, 'wb') as f:
                    torch.save(checkpoint, f)
                print(f"Saved checkpoint for task {task_id+1} at {checkpoint_path}")

    def load_checkpoint(self, model, checkpoint_path):
        if not os.path.exists(checkpoint_path):
            raise ValueError(f'체크포인트를 찾을 수 없습니다: {checkpoint_path}')
        
        print(f'체크포인트를 로드합니다: {checkpoint_path}')
        checkpoint = torch.load(checkpoint_path)
        
        model.load_state_dict(checkpoint['model'])

        return model