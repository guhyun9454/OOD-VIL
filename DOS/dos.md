이 문서는 **"DOS: Diverse Outlier Sampling for Out-of-Distribution Detection"**이라는 제목의 ICLR 2024 컨퍼런스 논문입니다1.

이 논문은 기계 학습 모델이 학습하지 않은 분포의 데이터(Out-of-Distribution, OOD)를 만났을 때 과도하게 높은 신뢰도를 보이는 문제를 해결하기 위해, **다양성(Diversity)**을 고려한 새로운 아웃라이어 샘플링 전략인 DOS를 제안합니다2222.
+1

논문의 핵심 내용을 배경, 문제점, 해결 방안(DOS), 그리고 실험 결과로 나누어 설명해 드리겠습니다.
1. 연구 배경 및 문제점
OOD 탐지 성능을 높이기 위해, 학습 중에 대규모의 보조 아웃라이어 데이터셋(Auxiliary OOD Dataset)을 활용하는 것이 일반적입니다3. 기존 연구들은 이 데이터셋에서 모델이 가장 헷갈려 하는(불확실성이 높은) 샘플만을 선택하는 '탐욕적 샘플링(Greedy Sampling)' 방식을 주로 사용했습니다4.
+1

하지만 저자들은 이러한 방식에 다음과 같은 문제가 있다고 지적합니다.
샘플링 편향: 불확실성(Uncertainty)만을 기준으로 선택하면 특정 유형이나 영역의 데이터만 선택되어 전체 아웃라이어 분포를 제대로 반영하지 못합니다5555.
+1


로컬 최적화: 결과적으로 결정 경계(Decision Boundary)가 국소적으로만 형성되어, 전반적인 OOD 탐지 성능이 떨어집니다6.


2. 제안 방법: DOS (Diverse Outlier Sampling)
저자들은 **다양성(Diversity)**이 OOD 탐지 성능에 결정적이라는 점을 실험적으로 증명하고, 이를 바탕으로 DOS 전략을 제안합니다7. DOS는 아웃라이어의 다양성을 확보하면서도 정보량이 많은 샘플을 선택하여 전역적으로 촘촘한(Globally compact) 결정 경계를 형성합니다8.
+1

DOS의 핵심 과정은 다음과 같습니다.
Step 1: 정규화된 특징 기반 클러스터링 (Clustering with Normalized Features)
먼저 후보 아웃라이어들을 K-means 알고리즘을 사용하여 여러 개의 클러스터로 그룹화합니다. 이때, 특징(Feature)의 크기(Scale)가 클러스터링에 편향을 주는 것을 막기 위해 특징을 정규화(Normalize)하여 사용합니다9999.
정규화된 K-means 알고리즘의 목적 함수는 다음과 같습니다.
+2

$$\text{arg min}_{C} \sum_{i=1}^{k} \sum_{x \in C_i} \left\| \frac{z}{\|z\|} - \mu_i \right\|^2$$
Step 2: 클러스터 내 능동적 샘플링 (Active Sampling in Each Cluster)
각 클러스터가 형성되면, 단순히 무작위로 뽑는 것이 아니라 각 클러스터 내에서 모델에게 가장 정보량이 많은(가장 헷갈리는) 아웃라이어를 선택합니다10101010.
이를 위해 '부재 클래스 확률(Absent category probability)'을 사용하며, 클러스터 $C_i$에서 가장 점수가 높은 샘플 $x_j$를 선택하는 수식은 다음과 같습니다.
+1

$$\text{arg max}_{j} [1.0 - p(K+1|x_j)]$$
Step 3: 모델 학습 (Training Objective)
선택된 다양한 아웃라이어들과 기존 학습 데이터(ID)를 함께 사용하여 모델을 학습시킵니다.
전체 손실 함수는 내부 데이터에 대한 크로스 엔트로피와 선택된 아웃라이어에 대한 손실의 합으로 정의됩니다.


$$\mathcal{L} = \mathbb{E}_{(x,y)\sim\mathcal{D}_{in}^{train}}[-y \log p(y|x)] + \mathbb{E}_{x\sim\mathcal{D}_{out}^{sam}}[-\log p(K+1|x)]$$
3. 주요 실험 결과
DOS는 다양한 벤치마크 데이터셋에서 기존 방법론(NTOM, POEM, Energy loss 등)을 크게 앞서는 성능을 보였습니다11111111.
+1

CIFAR-100 벤치마크: TI-300K를 보조 데이터셋으로 사용했을 때, 평균 FPR95(낮을수록 좋음)를 기존 NTOM 방식 대비 약 25.79% 포인트나 감소시켰습니다 (50.15% $\rightarrow$ 24.36%)12121212.
+1


대규모 데이터셋 적용: ImageNet-1K와 같은 대규모 데이터셋에서도 최첨단(SOTA) 성능을 달성했습니다13131313.
+1


효율성: 클러스터링 과정이 추가되었음에도 불구하고, 모델이 더 빠르게 수렴(Convergence)하여 전체적인 학습 효율성이 높음을 확인했습니다14.


4. 결론
이 논문은 OOD 탐지를 위한 샘플링 전략에서 다양성이 핵심 요소임을 밝혀냈습니다. 제안된 DOS는 정규화된 클러스터링과 정보 기반 샘플링을 결합하여, 보조 데이터셋의 잠재력을 최대한 활용하면서도 효율적으로 모델을 학습시키는 방법입니다15151515.
+1

이 논문의 방법론을 적용해 보고 싶은 데이터셋이나 모델 구조가 있으신가요?

1. 전체 파이프라인 개요DOS는 일반적인 학습 루프(Epoch/Iteration) 내에서 아웃라이어 선별 과정이 추가되는 형태입니다.입력: ID 데이터 배치($\mathcal{B}_{in}$), 보조 OOD 데이터 후보군($\mathcal{B}_{out}^{can}$)과정: 특징 추출 $\rightarrow$ 정규화 $\rightarrow$ K-Means 클러스터링 $\rightarrow$ 클러스터별 대표 샘플 선정 $\rightarrow$ 학습출력: 학습된 모델 파라미터 업데이트2. 단계별 구현 가이드Step 1: 특징 추출 및 정규화 (Feature Extraction & Normalization)현재 학습 중인 모델을 사용하여 보조 OOD 데이터 후보군($\mathcal{B}_{out}^{can}$)의 특징 벡터(Feature Vector)를 추출해야 합니다1. 여기서 특징 벡터는 분류기(Classifier)의 마지막 레이어(Logit) 직전의 출력을 의미합니다22.+2중요한 점은 추출된 특징 벡터를 그대로 사용하지 않고 단위 벡터로 정규화(Normalize) 해야 한다는 것입니다3333. 이는 특징의 크기(Scale)가 클러스터링에 미치는 편향을 줄이기 위함입니다.+1구현 포인트:L2 Norm을 사용하여 벡터를 정규화합니다. 정규화된 특징 $z_{norm}$은 다음과 같이 계산합니다.$$z_{norm} = \frac{z}{\|z\|}$$4Step 2: K-Means 클러스터링 (Clustering)정규화된 특징들을 K-Means 알고리즘을 사용하여 $K$개의 클러스터로 그룹화합니다5. 이때 클러스터의 개수 $K$는 ID 데이터의 배치 크기(Batch Size)와 동일하게 설정하는 것이 기본 설정입니다6666.+2구현 포인트:Sklearn의 KMeans 혹은 Faiss와 같은 라이브러리를 활용하여 구현할 수 있습니다.목적 함수는 각 데이터 포인트와 해당 클러스터 중심($\mu_i$) 간의 유클리드 거리 제곱 합을 최소화하는 것입니다.$$\text{arg min}_{C} \sum_{i=1}^{k} \sum_{x \in C_i} \left\| \frac{z}{\|z\|} - \mu_i \right\|^2$$7Step 3: 클러스터 내 하드 아웃라이어 선정 (Active Sampling)각 클러스터가 형성되면, 단순히 무작위로 선택하는 것이 아니라 모델이 가장 헷갈려 하는(정보량이 많은) 샘플 하나씩을 선택합니다8888.+1선택 기준:논문에서는 **부재 클래스 확률(Absent Category Probability)**이 가장 낮은, 즉 모델이 OOD라고 확신하지 못하는 샘플을 선택합니다9. 이는 $K+1$번째 클래스(OOD 클래스)에 대한 확률 $p(K+1|x)$를 이용하여 계산합니다.클러스터 $C_i$에서 선택될 샘플 $x_j$를 찾는 수식은 다음과 같습니다.$$\text{arg max}_{j} [1.0 - p(K+1|x_j)]$$10참고: 만약 $K+1$ 클래스를 별도로 두지 않는 모델이라면, 최대 로짓(Max Logit)이나 에너지 점수(Energy Score)가 높은(ID 데이터와 유사한) 샘플을 선택하는 방식으로 대체할 수 있습니다11.Step 4: 손실 함수 계산 및 학습 (Training Objective)선택된 아웃라이어 배치($\mathcal{B}_{out}^{train}$)와 원본 ID 데이터 배치($\mathcal{B}_{in}^{train}$)를 사용하여 모델을 업데이트합니다12.손실 함수:ID 데이터: 정답 레이블($y$)에 대한 Cross-Entropy LossOOD 데이터: $K+1$번째 클래스(OOD)로 분류되도록 하는 Loss$$\mathcal{L} = \mathbb{E}_{(x,y)\sim\mathcal{D}_{in}^{train}}[-y \log p(y|x)] + \mathbb{E}_{x\sim\mathcal{D}_{out}^{sam}}[-\log p(K+1|x)]$$133. 하이퍼파라미터 및 설정 요약코드를 작성할 때 참고할 기본 설정값입니다.보조 데이터셋 후보군 크기: 전체 보조 데이터셋을 한 번에 클러스터링하면 너무 느리므로, 미니 배치 스킴을 사용합니다14.Tip: 학습 루프 내에서 OOD 데이터를 Candidate Batch만큼 불러온 뒤, 이를 클러스터링하여 Final Batch를 선별합니다.클러스터 개수 ($k$): ID 데이터의 배치 크기(Batch Size)와 동일하게 설정합니다15151515.+1예: ID 배치 크기가 64라면, 아웃라이어 클러스터도 64개를 생성하여 각 클러스터에서 1개씩 뽑아 총 64개의 아웃라이어를 학습에 사용.학습 비율: ID 데이터와 선별된 OOD 데이터의 비율은 1:1로 설정합니다16.